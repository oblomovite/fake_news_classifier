{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The goal of this project is to perform classification of written media as misinformation or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data\n",
    "The [notebook](./index.ipynb) uses a [script](./download_datasets.sh) to automatically fetch the data for the project. It can be run manually to inspect the data beforehand by executing the following command:\n",
    "```bash\n",
    "chmod +x download_datasets.sh\n",
    "sh download_datasets.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and downloading datasets...\n",
      "Downloading Kaggle datasets...\n",
      "✔ fake-news-classification.zip already exists. Skipping download.\n",
      "✔ fake-and-real-news-dataset.zip already exists. Skipping download.\n",
      "Extracting Kaggle datasets...\n",
      "Downloading additional datasets...\n",
      "✔ liar_dataset.zip already exists. Skipping download.\n",
      "✔ All datasets are ready!\n"
     ]
    }
   ],
   "source": [
    "# fetch data using ./download_datasets.sh\n",
    "!bash ./download_datasets.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup for NLP tools\n",
    "In order to make full use of our NLP tooling we will install \n",
    "    - `punkt` for tokenization \n",
    "    - `stopwords` for removing common words\n",
    "    - `en_core_web_sm` for named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rob/micromamba/envs/fake-news/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /Users/rob/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rob/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt stopwords\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "The data is loaded, inspected, and assembled into a singular dataframe. The data is then preprocessed by removing stopwords, punctuation, and lemmatizing the text. The data is then split into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "The data is sourced from three different datasets:\n",
    "- [Fake & Real News]()\n",
    "- [Fake News Classification]()\n",
    "- [Liar Dataset]()\n",
    "\n",
    "The data is loaded, inspected, and assembled into a singular dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets paths\n",
    "datasets = {\n",
    "    \"fake_and_real_news\": {\n",
    "        \"fake\": \"datasets/fake-and-real-news/Fake.csv\",\n",
    "        \"real\": \"datasets/fake-and-real-news/True.csv\"\n",
    "    },\n",
    "    \"fake_news_classification\": {\n",
    "        \"train\": \"datasets/fake-news-classification/train (2).csv\",\n",
    "        \"test\": \"datasets/fake-news-classification/test (1).csv\",\n",
    "        \"evaluation\": \"datasets/fake-news-classification/evaluation.csv\"\n",
    "    },\n",
    "    \"liar_data\": {\n",
    "        \"train\": \"datasets/liar_data/train.tsv\",\n",
    "        \"test\": \"datasets/liar_data/test.tsv\",\n",
    "        \"valid\": \"datasets/liar_data/valid.tsv\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Fake & Real News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['headline', 'content', 'subject', 'date', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Fake & Real News Dataset\n",
    "df_fake = pd.read_csv(datasets[\"fake_and_real_news\"][\"fake\"])\n",
    "df_real = pd.read_csv(datasets[\"fake_and_real_news\"][\"real\"])\n",
    "\n",
    "# Assign labels\n",
    "df_fake[\"label\"] = \"fake\"\n",
    "df_real[\"label\"] = \"real\"\n",
    "\n",
    "# Standardize column names\n",
    "df_fake.rename(columns={\"title\": \"headline\", \"text\": \"content\"}, inplace=True)\n",
    "df_real.rename(columns={\"title\": \"headline\", \"text\": \"content\"}, inplace=True)\n",
    "\n",
    "# Merge Fake & Real News\n",
    "df_news = pd.concat([df_fake, df_real], ignore_index=True)\n",
    "\n",
    "# print columns\n",
    "print(df_news.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df_news.drop(columns=[\"subject\", \"date\", \"headline\"], inplace=True)\n",
    "\n",
    "# rename content to text\n",
    "df_news.rename(columns={\"content\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  Donald Trump just couldn t wish all Americans ...  fake\n",
      "1  House Intelligence Committee Chairman Devin Nu...  fake\n",
      "2  On Friday, it was revealed that former Milwauk...  fake\n",
      "3  On Christmas day, Donald Trump announced that ...  fake\n",
      "4  Pope Francis used his annual Christmas Day mes...  fake\n",
      "label\n",
      "fake    23481\n",
      "real    21417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print dataset info\n",
    "print(df_news.head())\n",
    "print(df_news[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Fake News Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load Fake News Classification Dataset with explicit delimiter\n",
    "df_train = pd.read_csv(datasets[\"fake_news_classification\"][\"train\"], delimiter=';')\n",
    "df_test = pd.read_csv(datasets[\"fake_news_classification\"][\"test\"], delimiter=';')\n",
    "df_evaluation = pd.read_csv(datasets[\"fake_news_classification\"][\"evaluation\"], delimiter=';')\n",
    "\n",
    "# Merge train, test, and evaluation datasets\n",
    "df_fake_news_class = pd.concat([df_train, df_test, df_evaluation], ignore_index=True)\n",
    "\n",
    "# print columns\n",
    "print(df_fake_news_class.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df_fake_news_class.drop(columns=['Unnamed: 0', 'title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  RAMALLAH, West Bank (Reuters) - Palestinians s...      1\n",
      "1  BEIJING (Reuters) - U.S. President-elect Donal...      1\n",
      "2  While the controversy over Trump s personal ta...      0\n",
      "3  BEIJING (Reuters) - A trip to Beijing last wee...      1\n",
      "4  There has never been a more UNCOURAGEOUS perso...      0\n",
      "label\n",
      "1    21924\n",
      "0    18663\n",
      "Name: count, dtype: int64\n",
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print dataset info\n",
    "print(df_fake_news_class.head())\n",
    "print(df_fake_news_class[\"label\"].value_counts())\n",
    "print(df_fake_news_class.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Liar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0  fake  Says the Annies List political group supports ...\n",
      "1  real  When did the decline of coal start? It started...\n",
      "2  real  Hillary Clinton agrees with John McCain \"by vo...\n",
      "3  fake  Health care reform legislation is likely to ma...\n",
      "4  real  The economic turnaround started at the end of ...\n",
      "label\n",
      "real    7134\n",
      "fake    5657\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define correct column names (LIAR dataset has 14 columns)\n",
    "columns = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\",\n",
    "           \"venue\", \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"context\"]\n",
    "\n",
    "# Load datasets with correct delimiter and column assignment\n",
    "df_liar_train = pd.read_csv(datasets[\"liar_data\"][\"train\"], delimiter='\\t', names=columns, header=None)\n",
    "df_liar_test = pd.read_csv(datasets[\"liar_data\"][\"test\"], delimiter='\\t', names=columns, header=None)\n",
    "df_liar_valid = pd.read_csv(datasets[\"liar_data\"][\"valid\"], delimiter='\\t', names=columns, header=None)\n",
    "\n",
    "# Combine datasets\n",
    "df_liar = pd.concat([df_liar_train, df_liar_test, df_liar_valid], ignore_index=True)\n",
    "\n",
    "# Map multi-class labels to binary labels\n",
    "label_mapping = {\n",
    "    \"pants-fire\": \"fake\",\n",
    "    \"false\": \"fake\",\n",
    "    \"barely-true\": \"fake\",\n",
    "    \"half-true\": \"real\",\n",
    "    \"mostly-true\": \"real\",\n",
    "    \"true\": \"real\"\n",
    "}\n",
    "df_liar[\"label\"] = df_liar[\"label\"].map(label_mapping)\n",
    "\n",
    "# Rename \"statement\" → \"content\" to match other datasets\n",
    "df_liar.rename(columns={\"statement\": \"content\"}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"id\", \"speaker\", \"job\", \"state\", \"subject\", \"party\", \"venue\", \n",
    "                   \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"context\"]\n",
    "df_liar.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# standardize column names\n",
    "df_liar.rename(columns={\"content\": \"text\"}, inplace=True)\n",
    "\n",
    "# Print dataset info\n",
    "print(df_liar.head())\n",
    "print(df_liar[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_liar.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Data\n",
    "Here we assemble the data into a singular dataframe. This will involve renaming columns, dropping unnecessary columns, and adding a label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "real    50475\n",
      "fake    47801\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# standardize labels\n",
    "df_fake_news_class[\"label\"] = df_fake_news_class[\"label\"].map({1: \"real\", 0: \"fake\"})\n",
    "\n",
    "# ensure all labels are strings\n",
    "df_liar[\"label\"] = df_liar[\"label\"].astype(str)\n",
    "df_fake_news_class[\"label\"] = df_fake_news_class[\"label\"].astype(str)\n",
    "df_news[\"label\"] = df_news[\"label\"].astype(str)\n",
    "\n",
    "# merge datasets\n",
    "df = pd.concat([df_news, df_fake_news_class, df_liar], ignore_index=True)\n",
    "\n",
    "# check standardized label distribution\n",
    "print(df[\"label\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
