{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The goal of this project is to perform classification of written media as misinformation or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data\n",
    "The [notebook](./index.ipynb) uses a [script](./download_datasets.sh) to automatically fetch the data for the project. It can be run manually to inspect the data beforehand by executing the following command:\n",
    "```bash\n",
    "chmod +x download_datasets.sh\n",
    "sh download_datasets.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and downloading datasets...\n",
      "Downloading Kaggle datasets...\n",
      "✔ fake-news-classification.zip already exists. Skipping download.\n",
      "✔ fake-and-real-news-dataset.zip already exists. Skipping download.\n",
      "Extracting Kaggle datasets...\n",
      "Downloading additional datasets...\n",
      "✔ liar_dataset.zip already exists. Skipping download.\n",
      "✔ All datasets are ready!\n"
     ]
    }
   ],
   "source": [
    "# fetch data using ./download_datasets.sh\n",
    "!bash ./download_datasets.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup for NLP tools\n",
    "In order to make full use of our NLP tooling we will install:\n",
    "- `punkt` for tokenization \n",
    "- `stopwords` for removing common words\n",
    "- `wordnet` for lemmatization\n",
    "- `averaged_perceptron_tagger` for part of speech tagging\n",
    "- `maxent_ne_chunker` for named entity recognition\n",
    "- `words` for named entity recognition\n",
    "- `spacy` for named entity recognition\n",
    "- `en_core_web_sm` for named entity recognition\n",
    "```bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rob/micromamba/envs/fake-news/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /Users/rob/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rob/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt stopwords wordnet averaged_perceptron_tagger\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "The data is loaded, inspected, and assembled into a singular dataframe. The data is then preprocessed by removing stopwords, punctuation, and lemmatizing the text. The data is then split into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "The data is sourced from three different datasets:\n",
    "- [Fake & Real News]()\n",
    "- [Fake News Classification]()\n",
    "- [Liar Dataset]()\n",
    "\n",
    "The data is loaded, inspected, and assembled into a singular dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets paths\n",
    "datasets = {\n",
    "    \"fake_and_real_news\": {\n",
    "        \"fake\": \"datasets/fake-and-real-news/Fake.csv\",\n",
    "        \"real\": \"datasets/fake-and-real-news/True.csv\"\n",
    "    },\n",
    "    \"fake_news_classification\": {\n",
    "        \"train\": \"datasets/fake-news-classification/train (2).csv\",\n",
    "        \"test\": \"datasets/fake-news-classification/test (1).csv\",\n",
    "        \"evaluation\": \"datasets/fake-news-classification/evaluation.csv\"\n",
    "    },\n",
    "    \"liar_data\": {\n",
    "        \"train\": \"datasets/liar_data/train.tsv\",\n",
    "        \"test\": \"datasets/liar_data/test.tsv\",\n",
    "        \"valid\": \"datasets/liar_data/valid.tsv\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Fake & Real News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['headline', 'content', 'subject', 'date', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Fake & Real News Dataset\n",
    "df_fake = pd.read_csv(datasets[\"fake_and_real_news\"][\"fake\"])\n",
    "df_real = pd.read_csv(datasets[\"fake_and_real_news\"][\"real\"])\n",
    "\n",
    "# Assign labels\n",
    "df_fake[\"label\"] = \"fake\"\n",
    "df_real[\"label\"] = \"real\"\n",
    "\n",
    "# Standardize column names\n",
    "df_fake.rename(columns={\"title\": \"headline\", \"text\": \"content\"}, inplace=True)\n",
    "df_real.rename(columns={\"title\": \"headline\", \"text\": \"content\"}, inplace=True)\n",
    "\n",
    "# Merge Fake & Real News\n",
    "df_news = pd.concat([df_fake, df_real], ignore_index=True)\n",
    "\n",
    "# print columns\n",
    "print(df_news.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df_news.drop(columns=[\"subject\", \"date\", \"headline\"], inplace=True)\n",
    "\n",
    "# rename content to text\n",
    "df_news.rename(columns={\"content\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  Donald Trump just couldn t wish all Americans ...  fake\n",
      "1  House Intelligence Committee Chairman Devin Nu...  fake\n",
      "2  On Friday, it was revealed that former Milwauk...  fake\n",
      "3  On Christmas day, Donald Trump announced that ...  fake\n",
      "4  Pope Francis used his annual Christmas Day mes...  fake\n",
      "label\n",
      "fake    23481\n",
      "real    21417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print dataset info\n",
    "print(df_news.head())\n",
    "print(df_news[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Fake News Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load Fake News Classification Dataset with explicit delimiter\n",
    "df_train = pd.read_csv(datasets[\"fake_news_classification\"][\"train\"], delimiter=';')\n",
    "df_test = pd.read_csv(datasets[\"fake_news_classification\"][\"test\"], delimiter=';')\n",
    "df_evaluation = pd.read_csv(datasets[\"fake_news_classification\"][\"evaluation\"], delimiter=';')\n",
    "\n",
    "# Merge train, test, and evaluation datasets\n",
    "df_fake_news_class = pd.concat([df_train, df_test, df_evaluation], ignore_index=True)\n",
    "\n",
    "# print columns\n",
    "print(df_fake_news_class.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df_fake_news_class.drop(columns=['Unnamed: 0', 'title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  RAMALLAH, West Bank (Reuters) - Palestinians s...      1\n",
      "1  BEIJING (Reuters) - U.S. President-elect Donal...      1\n",
      "2  While the controversy over Trump s personal ta...      0\n",
      "3  BEIJING (Reuters) - A trip to Beijing last wee...      1\n",
      "4  There has never been a more UNCOURAGEOUS perso...      0\n",
      "label\n",
      "1    21924\n",
      "0    18663\n",
      "Name: count, dtype: int64\n",
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print dataset info\n",
    "print(df_fake_news_class.head())\n",
    "print(df_fake_news_class[\"label\"].value_counts())\n",
    "print(df_fake_news_class.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Liar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0  fake  Says the Annies List political group supports ...\n",
      "1  real  When did the decline of coal start? It started...\n",
      "2  real  Hillary Clinton agrees with John McCain \"by vo...\n",
      "3  fake  Health care reform legislation is likely to ma...\n",
      "4  real  The economic turnaround started at the end of ...\n",
      "label\n",
      "real    7134\n",
      "fake    5657\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define correct column names (LIAR dataset has 14 columns)\n",
    "columns = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job\", \"state\", \"party\",\n",
    "           \"venue\", \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"context\"]\n",
    "\n",
    "# Load datasets with correct delimiter and column assignment\n",
    "df_liar_train = pd.read_csv(datasets[\"liar_data\"][\"train\"], delimiter='\\t', names=columns, header=None)\n",
    "df_liar_test = pd.read_csv(datasets[\"liar_data\"][\"test\"], delimiter='\\t', names=columns, header=None)\n",
    "df_liar_valid = pd.read_csv(datasets[\"liar_data\"][\"valid\"], delimiter='\\t', names=columns, header=None)\n",
    "\n",
    "# Combine datasets\n",
    "df_liar = pd.concat([df_liar_train, df_liar_test, df_liar_valid], ignore_index=True)\n",
    "\n",
    "# Map multi-class labels to binary labels\n",
    "label_mapping = {\n",
    "    \"pants-fire\": \"fake\",\n",
    "    \"false\": \"fake\",\n",
    "    \"barely-true\": \"fake\",\n",
    "    \"half-true\": \"real\",\n",
    "    \"mostly-true\": \"real\",\n",
    "    \"true\": \"real\"\n",
    "}\n",
    "df_liar[\"label\"] = df_liar[\"label\"].map(label_mapping)\n",
    "\n",
    "# Rename \"statement\" → \"content\" to match other datasets\n",
    "df_liar.rename(columns={\"statement\": \"content\"}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"id\", \"speaker\", \"job\", \"state\", \"subject\", \"party\", \"venue\", \n",
    "                   \"barely-true\", \"false\", \"half-true\", \"mostly-true\", \"pants-fire\", \"context\"]\n",
    "df_liar.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# standardize column names\n",
    "df_liar.rename(columns={\"content\": \"text\"}, inplace=True)\n",
    "\n",
    "# Print dataset info\n",
    "print(df_liar.head())\n",
    "print(df_liar[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_liar.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Data\n",
    "Here we assemble the data into a singular dataframe. This will involve renaming columns, dropping unnecessary columns, and adding a label column. Importantly we will set our target variable to be binary, with 1 representing misinformation and 0 representing accurate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    50475\n",
      "1    47801\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# standardize labels in datasets -- convert label to 1 (`fake`) and 0 (`real`)\n",
    "df_liar[\"label\"] = df_liar[\"label\"].map({\"fake\": 1, \"real\": 0})\n",
    "df_fake_news_class[\"label\"] = df_fake_news_class[\"label\"].map({\"fake\": 1, \"real\": 0})\n",
    "df_news[\"label\"] = df_news[\"label\"].map({\"fake\": 1, \"real\": 0})\n",
    "\n",
    "# merge datasets\n",
    "df = pd.concat([df_news, df_fake_news_class, df_liar], ignore_index=True)\n",
    "\n",
    "# check standardized label distribution\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Our preprocessing steps will involve\n",
    "- removing stopwords\n",
    "- removing punctuation\n",
    "- lemmatization (converting words to their base form)\n",
    "- removing special characters, numbers, and extra spaces\n",
    "- converting text to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Special Characters (numbers, punctuation, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    function to format and clean text by lowercasing text, removing URLs, numbers, and punctuation. \n",
    "    '''\n",
    "    text = text.lower() \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# apply text cleaning function to text column\n",
    "df[\"text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords\n",
    "Stopwords are common words that do not add much meaning (i.e. articles, prepositions, etc.) to a sentence and can safely be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# apply remove_stopwords function to text column\n",
    "df[\"text\"] = df[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "This process reduces words to their base form (e.g., \"running\" → \"run\") which can help reduce the complexity of the data and improve the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# apply lemmatize_text function to text column\n",
    "df[\"text\"] = df[\"text\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Data\n",
    "We shuffle the data to ensure that the model does not learn the order of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "df_final = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "We split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_final[\"text\"], df_final[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "After preprocessing the data, we need to convert the text data into a format that can be used by machine learning algorithms. We will use the TF-IDF vectorizer to convert the text data into numerical features. This will serve as our baseline model. Later, we will explore more advanced feature extraction techniques with word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. We will use the TF-IDF vectorizer to convert the text data into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to top 5000 words\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
